{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNfromScratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvchZ1awfvm_",
        "colab_type": "text"
      },
      "source": [
        "<h1>Making RNN from Scratch</h1>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMhI2dDVfyKn",
        "colab_type": "text"
      },
      "source": [
        "# Planning\n",
        "1) Initially, get the data \n",
        "\n",
        "2) make a dictionary containing values for each word.\n",
        "\n",
        "3) Convert each sentence into vector\n",
        "\n",
        "4) Make RNN architecture ->Define inputs and outputs of every neuron\n",
        "\n",
        "5) The input sentence vector goes into each layer and does forward pass ->Write Forward and Backward pass\n",
        "\n",
        "6) Make necessary changes for optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXMU8DWJPPeB",
        "colab_type": "text"
      },
      "source": [
        "# Task 1 Get the data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obr8-2_tfSVu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4f0a9908-14b5-4f8a-8506-5c1547eabef4"
      },
      "source": [
        "!unzip \"/content/drive/My Drive/Coursera Guided Projects/11827_16290_bundle_archive.zip\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Coursera Guided Projects/11827_16290_bundle_archive.zip\n",
            "  inflating: Womens Clothing E-Commerce Reviews.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmijGpsdDR5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from itertools import repeat"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9r7RZHRDhMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv('/content/Womens Clothing E-Commerce Reviews.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMtkL6mADoAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "24b23abb-f297-4fe3-9776-e3c4ccff8b70"
      },
      "source": [
        "df=data[['Clothing ID','Title','Review Text']]\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Clothing ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Review Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>767</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1077</td>\n",
              "      <td>Some major design flaws</td>\n",
              "      <td>I had such high hopes for this dress and reall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1049</td>\n",
              "      <td>My favorite buy!</td>\n",
              "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>847</td>\n",
              "      <td>Flattering shirt</td>\n",
              "      <td>This shirt is very flattering to all due to th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Clothing ID  ...                                        Review Text\n",
              "0          767  ...  Absolutely wonderful - silky and sexy and comf...\n",
              "1         1080  ...  Love this dress!  it's sooo pretty.  i happene...\n",
              "2         1077  ...  I had such high hopes for this dress and reall...\n",
              "3         1049  ...  I love, love, love this jumpsuit. it's fun, fl...\n",
              "4          847  ...  This shirt is very flattering to all due to th...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMKFDqzLPZVI",
        "colab_type": "text"
      },
      "source": [
        "# Task 2 make a dictionary containing values for each word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXGDFA_FT7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=list(df['Title'].dropna())\n",
        "y=list(df['Review Text'].dropna())\n",
        "#sentence_lengths=[]\n",
        "\n",
        "l1,l2=[],[]\n",
        "#Getting all the words in title and review text\n",
        "for i in range(len(y)):\n",
        "  if isinstance(y[i], int)==True:\n",
        "    continue\n",
        "  else:\n",
        "    y[i] = y[i].lower()\n",
        "    p = re.findall(r'(?:\\w+)', y[i], flags = re.UNICODE)\n",
        "    #sentence_lengths.append(len(p))\n",
        "    for j in range(len(p)): \n",
        "      l2.append(p[j]) \n",
        "\n",
        "for i in range(len(x)):\n",
        "  if isinstance(x[i], int)==True:\n",
        "    continue\n",
        "  else:\n",
        "    x[i] = x[i].lower()\n",
        "    p = re.findall(r'(?:\\w+)', x[i], flags = re.UNICODE)\n",
        "    for j in range(len(p)): \n",
        "      l1.append(p[j]) \n",
        "\n",
        "#taking unique values only\n",
        "keys=set(l1).union(set(l2))\n",
        "\n",
        "#making a dictionary for vectorization\n",
        "dictionary={key: value for value,key in enumerate(keys)}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tL3Hzb_DpDw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "868db095-4ae6-4f69-c76f-ce06edc660a0"
      },
      "source": [
        "#making vectors of the sentences\n",
        "#checking average length of the sentences\n",
        "\n",
        "sentence_lengths=[]\n",
        "for i in range(len(y)):\n",
        "    p = re.findall(r'(?:\\w+)', y[i], flags = re.UNICODE)\n",
        "    sentence_lengths.append(len(p))\n",
        "    \n",
        "plt.hist(sentence_lengths,bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHBmIMlJOvHn",
        "colab_type": "text"
      },
      "source": [
        "Let us consider the average length of Sentences as 100 and continue to make vector of length 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqQOiOglPgLM",
        "colab_type": "text"
      },
      "source": [
        "# Task 3  Convert each sentence into vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xqUFYq6RjLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adding capital o at the end of the dictionary\n",
        "dictionary['O']=len(dictionary)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhV7-aSISx_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generating empty array to store values\n",
        "sentence_vectors=np.empty((100))\n",
        "sentence_vectors=np.expand_dims(sentence_vectors,axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2y4bYeI58U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#making a single vector for all the sentences\n",
        "for i in y:\n",
        "  p = re.findall(r'(?:\\w+)', i, flags = re.UNICODE)\n",
        "  if len(p)<100:\n",
        "    p.extend(repeat('O',100-len(p)))\n",
        "  words=p[:100]\n",
        "  vector=[dictionary[word] for word in words] \n",
        "  x=np.expand_dims(np.array(vector),axis=0) \n",
        "  sentence_vectors=np.concatenate((sentence_vectors,x),axis=0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4uBh9EATWW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11141e84-ad5d-498c-950c-d0a33c442b72"
      },
      "source": [
        "#removing the extra row created while initialization\n",
        "sentence_vectors = np.delete(sentence_vectors, 0, 0)\n",
        "sentence_vectors.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22641, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cct-H2yYZmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_of_layers=100\n",
        "vocabsize=14718\n",
        "seq_length=100 #Tx,Ty\n",
        "num_examples=22641\n",
        "no_of_neurons=4 #neurons for wach layer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0omracjrBWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert into one hot vector\n",
        "\n",
        "#alldata_sentences=np.zeros((num_examples,vocabsize,no_of_layers))\n",
        "#for i in range(sentence_vectors.shape[0]):\n",
        "def get_one_hot_encoded_vector(i):  \n",
        "  sentence=np.zeros((vocabsize,no_of_layers))\n",
        "  count=0\n",
        "  for a in sentence_vectors[i][:]:\n",
        "    my_vec=np.zeros((vocabsize))\n",
        "    my_vec[int(a)]=1\n",
        "    sentence[:,count]=my_vec\n",
        "    count+=1\n",
        "  return sentence\n",
        "  #print('o')\n",
        "  #alldata_sentences[i]=sentence"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM7xBqv-Xe2V",
        "colab_type": "text"
      },
      "source": [
        "# Task 4 Make RNN architecture ->Define inputs and outputs of every neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HidKEBflWNa",
        "colab_type": "text"
      },
      "source": [
        "need of total 100 layers, as each layer will be given a single word.\n",
        "\n",
        "Each word of a sentence when goes into the RNN, needs to be one hot encoded\n",
        "\n",
        "no_of_neurons=100, Vocab_Size=14718,\n",
        " Seq_length=100, num_examples=22641\n",
        "\n",
        "Shapes:\n",
        "no_of_neurons=4\n",
        "\n",
        "x = (vocabsize,no_of_layers)\n",
        "\n",
        "y1 = (vocabsize,no_of_layers)\n",
        "\n",
        "a = (no_of_neurons,no_of_layers+1)\n",
        "\n",
        "waa = (no_of_neurons,no_of_layers)\n",
        "\n",
        "wya = (vocabsize,no_of_neurons,no_of_layers) \n",
        "\n",
        "wax = (no_of_neurons,vocabsize,no_of_layers) \n",
        "\n",
        "ba = (1,no_of_layers)\n",
        "\n",
        "by = (1,no_of_layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAurXiTDX9Ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0) "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_VFDt68JGSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#newer \n",
        "def forward_pass(a,x,wax,wya,waa,ba,by):\n",
        "  #at timestep t\n",
        "\n",
        "  #a[t],x[t]-->a,x\n",
        "  #a[t+1],y[t+1]-->a_next,y_next\n",
        "  a_next = np.tanh(np.dot(waa,a.T)+np.dot(wax,x.T)+ba)\n",
        "  s_inp=np.dot(wya,a)+by\n",
        "  y_next = softmax(s_inp)\n",
        "  loss1 = - x*np.log(y_next) - ((1-x)*np.log(1-y_next))#y[t]=x[t] and ycap[t]=y[t+1]\n",
        "  loss2 = a_next*np.log(a) #y=a[t+1] and ycap=a[t]\n",
        "  #print(a)\n",
        "  #print(np.log(a))\n",
        "  return a_next,y_next,loss1,loss2\n",
        "\n",
        "def backward_pass(a,a_next,y_next,x):\n",
        "  #a[t],x[t],wax,wya,waa,ba,by-->a,x\n",
        "  #a[t+1],y[t+1]-->a_next,y_next\n",
        "  \n",
        "  #differentiation of loss function wrt a[t]\n",
        "  da = np.log(a) #dloss2/da[t+1]\n",
        "  dy = np.divide(x,y_next) - np.dot( np.log(1-y_next) , (1-np.divide(x,1-y_next)) )#dloss1/dy[t+1])\n",
        "  dwaa = np.multiply(da,np.multiply(1-np.square(a_next),a)) #dloss2/dwaa waa of time step t\n",
        "  new_a_next=np.expand_dims(a_next,axis=1)\n",
        "  new_x=np.expand_dims(x,axis=1)\n",
        "  new_da=np.expand_dims(da,axis=1)\n",
        "  dwax = np.multiply(new_da,np.dot(1-np.square(new_a_next),new_x.T)) #dloss2/dwax\n",
        "  dwya = np.multiply(dy[np.newaxis].T,np.multiply(y_next[np.newaxis].T,np.dot((1-y_next)[np.newaxis].T,a[np.newaxis]))) #dloss1/dwya ->dloss1/dy*dy/dwya\n",
        "  dba = np.dot(da[np.newaxis],(1-np.square(a_next))[np.newaxis].T) #dloss2/dba\n",
        "  dby = np.dot(dy[np.newaxis],np.multiply(y_next,(1-y_next))[np.newaxis].T) #dloss1/dby\n",
        "  return dwaa,dwya,dwax,dba,dby"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgeVECCR9mOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def differentiation_relu(x):\n",
        "  x[x>0]=1\n",
        "  x[x<=0]=0\n",
        "  return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77PVXQkf7sv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#newer 2 \n",
        "def forward_pass(a,x,wax,wya,waa,ba,by):\n",
        "  #at timestep t\n",
        "\n",
        "  #a[t],x[t]-->a,x\n",
        "  #a[t+1],y[t+1]-->a_next,y_next\n",
        "  #a_next = np.tanh(np.dot(waa,a.T)+np.dot(wax,x.T)+ba)\n",
        "  a_next = np.maximum((np.dot(waa,a.T)+np.dot(wax,x.T)+ba),0)\n",
        "  s_inp=np.dot(wya,a)+by\n",
        "  y_next = softmax(s_inp)\n",
        "  loss1 = - x*np.log(y_next) - ((1-x)*np.log(1-y_next))#y[t]=x[t] and ycap[t]=y[t+1]\n",
        "  #loss2 = a_next*np.log(a) #y=a[t+1] and ycap=a[t]\n",
        "  return a_next,y_next,loss1,loss2\n",
        "\n",
        "def backward_pass(a,a_next,y_next,x):\n",
        "  #a[t],x[t],wax,wya,waa,ba,by-->a,x\n",
        "  #a[t+1],y[t+1]-->a_next,y_next\n",
        "  \n",
        "  #differentiation of loss function wrt a[t]\n",
        "  da = np.log(a) #dloss2/da[t+1]\n",
        "  dy = np.divide(x,y_next) - np.dot( np.log(1-y_next) , (1-np.divide(x,1-y_next)) )#dloss1/dy[t+1])\n",
        "  dwaa = np.multiply(da,np.multiply(differentiation_relu(a_next),a)) #dloss2/dwaa waa of time step t\n",
        "  new_a_next=np.expand_dims(a_next,axis=1)\n",
        "  new_x=np.expand_dims(x,axis=1)\n",
        "  new_da=np.expand_dims(da,axis=1)\n",
        "  dwax = np.multiply(new_da,np.dot(differentiation_relu(new_a_next),new_x.T)) #dloss2/dwax\n",
        "  dwya = np.multiply(dy[np.newaxis].T,np.multiply(y_next[np.newaxis].T,np.dot((1-y_next)[np.newaxis].T,a[np.newaxis]))) #dloss1/dwya ->dloss1/dy*dy/dwya\n",
        "  dba = np.dot(da[np.newaxis],(differentiation_relu(a_next))[np.newaxis].T) #dloss2/dba\n",
        "  dby = np.dot(dy[np.newaxis],np.multiply(y_next,(1-y_next))[np.newaxis].T) #dloss1/dby\n",
        "  return dwaa,dwya,dwax,dba,dby"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pepHC1U3kC4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_params_init():\n",
        "  #x=sentence#(vocabsize,no_of_layers)\n",
        "  x=np.zeros((vocabsize,no_of_layers))\n",
        "  y1=np.zeros((vocabsize,no_of_layers))\n",
        "  a = np.random.uniform(0, 1, (no_of_neurons,no_of_layers+1))\n",
        "  waa = np.random.uniform(0, 1, (no_of_neurons,no_of_layers)) \n",
        "  wya = np.random.uniform(0, 1, (vocabsize,no_of_neurons,no_of_layers)) \n",
        "  wax = np.random.uniform(0, 1, (no_of_neurons,vocabsize,no_of_layers)) \n",
        "  ba = np.random.uniform(0, 1, (1,no_of_layers))\n",
        "  by = np.random.uniform(0, 1, (1,no_of_layers))\n",
        "  outputs = np.zeros((vocabsize, no_of_layers))\n",
        "  return x,y1,a,waa,wya,wax,ba,by,outputs"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TCRUpgfiDpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the architecture of RNN\n",
        "def model_train(epochs,learning_rate):\n",
        "  x,y1,a,waa,wya,wax,ba,by,outputs=model_params_init()\n",
        "  for j in range(epochs):\n",
        "    total_loss1=[]\n",
        "    total_loss2=[]\n",
        "    count=0\n",
        "    for i in range(num_examples):\n",
        "      x=get_one_hot_encoded_vector(i)#(vocabsize,no_of_layers)\n",
        "\n",
        "      #forward pass for a single sentence\n",
        "      for t in range(0,no_of_layers):\n",
        "        print(a[:,t])\n",
        "        a[:,t+1],y1[:,t],loss1,loss2=forward_pass(a[:,t],x[:,t],wax[:,:,t],wya[:,:,t],waa[:,t],ba[0,t],by[0,t] )#perform forward pass at instant t\n",
        "        total_loss1.append(loss1)\n",
        "        total_loss2.append(loss2)\n",
        "        #forward pass should return y,a[t+1] and loss\n",
        "      \n",
        "      #backward pass for a single sentence\n",
        "      for t in range(no_of_layers-1,0,-1):\n",
        "        #t->t-1\n",
        "        dwaa,dwya,dwax,dba,dby=backward_pass(a[:,t-1],a[:,t],y1[:,t],x[:,t-1])\n",
        "        alpha=learning_rate\n",
        "        #backward pass will update weights of every layer --> need to add timestep to them, like update entire column no. t-1 (waa,wax,wya)\n",
        "        #weight updation\n",
        "        waa[:,t-1] = waa[:,t-1] + alpha*dwaa#(4,1)\n",
        "        wax[:,:,t-1] = wax[:,:,t-1] + alpha*dwax#(4,14718)\n",
        "        wya[:,:,t-1] = wya[:,:,t-1] + alpha*dwya#(14718,4)\n",
        "        ba[0,t-1] = ba[0,t-1] + alpha*dba\n",
        "        by[0,t-1] = by[0,t-1] + alpha*dby\n",
        "        #break\n",
        "      #break\n",
        "      print('example '+str(i)+'/'+str(num_examples)+' completed')\n",
        "      count+=1\n",
        "      if count==10:\n",
        "        break\n",
        "    print('epoch '+str(j)+' completed')\n",
        "    print('average vertical loss',sum(total_loss1)/(len(total_loss1)+1))\n",
        "    print('average horizontal loss',sum(total_loss2)/(len(total_loss2)+1))\n",
        "  return waa,wax,wya,ba,by,a\n",
        "  #return a,y1,x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHrP6k8j726",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02dd192c-9baf-4e24-b308-36a3d40ca6da"
      },
      "source": [
        "epochs=1\n",
        "learning_rate=0.3\n",
        "model=model_train(epochs,learning_rate)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[1.07789878 1.55837824 1.42741612 1.18110974]\n",
            "[4.89553964 5.42628306 5.55334768 5.51666208]\n",
            "[16.43639073 16.38548389 16.64781784 15.96081036]\n",
            "[48.47365991 47.85581554 47.90642589 47.96215553]\n",
            "[122.54777305 121.763673   121.86851363 121.94068408]\n",
            "[240.22300373 239.98260834 240.27590062 240.55302902]\n",
            "[349.62191934 349.43984258 349.82992394 349.74156658]\n",
            "[491.9054472  491.49551795 492.14800496 491.9715944 ]\n",
            "[780.42545127 780.16599997 780.38695535 780.79364519]\n",
            "[2899.02328091 2899.93400877 2899.79081519 2899.3400569 ]\n",
            "[6549.44325792 6549.47914557 6549.53170675 6549.71371893]\n",
            "[13461.14750315 13461.36457727 13461.48731807 13460.96941145]\n",
            "[15668.78391887 15669.31660044 15668.81147144 15669.47896561]\n",
            "[27943.51923778 27943.83237677 27943.89764394 27943.38192059]\n",
            "[59010.15474842 59010.00216999 59009.82640474 59009.78487032]\n",
            "[64836.63034388 64836.50843124 64836.89235411 64836.3637187 ]\n",
            "[155318.15189622 155317.51649376 155317.39921042 155317.46914541]\n",
            "[291525.47546868 291525.30460769 291525.64975459 291525.69428518]\n",
            "[754148.4899301  754147.63882235 754148.28867262 754147.91338852]\n",
            "[1896691.01683902 1896691.29179346 1896690.5545389  1896691.24533533]\n",
            "[4439351.44674261 4439351.72749811 4439350.89862469 4439351.28835164]\n",
            "[9481749.27634298 9481749.23277634 9481749.53480982 9481748.88712268]\n",
            "[20196154.4754961  20196154.56499635 20196154.1068284  20196153.73176844]\n",
            "[36252504.22109549 36252504.02114127 36252504.1789801  36252504.41533803]\n",
            "[62569684.42343517 62569684.24640903 62569684.15227926 62569683.89565241]\n",
            "[1.05132229e+08 1.05132228e+08 1.05132228e+08 1.05132229e+08]\n",
            "[94044177.23352574 94044176.87658675 94044176.4431839  94044176.6079942 ]\n",
            "[2.51319004e+08 2.51319004e+08 2.51319004e+08 2.51319005e+08]\n",
            "[5.38387084e+08 5.38387083e+08 5.38387084e+08 5.38387084e+08]\n",
            "[1.23995105e+09 1.23995105e+09 1.23995105e+09 1.23995105e+09]\n",
            "[3.78210167e+09 3.78210167e+09 3.78210166e+09 3.78210166e+09]\n",
            "[7.6100469e+09 7.6100469e+09 7.6100469e+09 7.6100469e+09]\n",
            "[1.60097124e+10 1.60097124e+10 1.60097124e+10 1.60097124e+10]\n",
            "[3.04000823e+10 3.04000823e+10 3.04000823e+10 3.04000823e+10]\n",
            "[7.33364339e+10 7.33364339e+10 7.33364339e+10 7.33364339e+10]\n",
            "[1.51192998e+11 1.51192998e+11 1.51192998e+11 1.51192998e+11]\n",
            "[4.37870623e+11 4.37870623e+11 4.37870623e+11 4.37870623e+11]\n",
            "[1.16260745e+12 1.16260745e+12 1.16260745e+12 1.16260745e+12]\n",
            "[2.7172706e+12 2.7172706e+12 2.7172706e+12 2.7172706e+12]\n",
            "[7.68121668e+12 7.68121668e+12 7.68121668e+12 7.68121668e+12]\n",
            "[8.65338756e+12 8.65338756e+12 8.65338756e+12 8.65338756e+12]\n",
            "[1.65784024e+13 1.65784024e+13 1.65784024e+13 1.65784024e+13]\n",
            "[5.03769849e+13 5.03769849e+13 5.03769849e+13 5.03769849e+13]\n",
            "[5.13165517e+13 5.13165517e+13 5.13165517e+13 5.13165517e+13]\n",
            "[1.19772699e+14 1.19772699e+14 1.19772699e+14 1.19772699e+14]\n",
            "[1.98100067e+14 1.98100067e+14 1.98100067e+14 1.98100067e+14]\n",
            "[2.48239695e+14 2.48239695e+14 2.48239695e+14 2.48239695e+14]\n",
            "[4.17939359e+14 4.17939359e+14 4.17939359e+14 4.17939359e+14]\n",
            "[9.92915221e+14 9.92915221e+14 9.92915221e+14 9.92915221e+14]\n",
            "[2.61274666e+15 2.61274666e+15 2.61274666e+15 2.61274666e+15]\n",
            "[5.8963438e+15 5.8963438e+15 5.8963438e+15 5.8963438e+15]\n",
            "[1.62467871e+16 1.62467871e+16 1.62467871e+16 1.62467871e+16]\n",
            "[2.10551224e+16 2.10551224e+16 2.10551224e+16 2.10551224e+16]\n",
            "[5.13235511e+16 5.13235511e+16 5.13235511e+16 5.13235511e+16]\n",
            "[5.63997408e+16 5.63997408e+16 5.63997408e+16 5.63997408e+16]\n",
            "[6.72951856e+16 6.72951856e+16 6.72951856e+16 6.72951856e+16]\n",
            "[1.18178487e+17 1.18178487e+17 1.18178487e+17 1.18178487e+17]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in log\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in multiply\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2.44270943e+17 2.44270943e+17 2.44270943e+17 2.44270943e+17]\n",
            "[5.07900501e+17 5.07900501e+17 5.07900501e+17 5.07900501e+17]\n",
            "[1.23153216e+18 1.23153216e+18 1.23153216e+18 1.23153216e+18]\n",
            "[3.6293113e+18 3.6293113e+18 3.6293113e+18 3.6293113e+18]\n",
            "[5.73198866e+18 5.73198866e+18 5.73198866e+18 5.73198866e+18]\n",
            "[1.33720497e+19 1.33720497e+19 1.33720497e+19 1.33720497e+19]\n",
            "[2.52060731e+19 2.52060731e+19 2.52060731e+19 2.52060731e+19]\n",
            "[2.27459886e+19 2.27459886e+19 2.27459886e+19 2.27459886e+19]\n",
            "[5.17408718e+19 5.17408718e+19 5.17408718e+19 5.17408718e+19]\n",
            "[8.04449513e+19 8.04449513e+19 8.04449513e+19 8.04449513e+19]\n",
            "[2.0563415e+20 2.0563415e+20 2.0563415e+20 2.0563415e+20]\n",
            "[4.59404704e+20 4.59404704e+20 4.59404704e+20 4.59404704e+20]\n",
            "[1.00298932e+21 1.00298932e+21 1.00298932e+21 1.00298932e+21]\n",
            "[1.55190096e+21 1.55190096e+21 1.55190096e+21 1.55190096e+21]\n",
            "[5.01463221e+21 5.01463221e+21 5.01463221e+21 5.01463221e+21]\n",
            "[5.20501377e+21 5.20501377e+21 5.20501377e+21 5.20501377e+21]\n",
            "[1.05654143e+22 1.05654143e+22 1.05654143e+22 1.05654143e+22]\n",
            "[1.71132474e+22 1.71132474e+22 1.71132474e+22 1.71132474e+22]\n",
            "[4.52756699e+22 4.52756699e+22 4.52756699e+22 4.52756699e+22]\n",
            "[8.26580877e+22 8.26580877e+22 8.26580877e+22 8.26580877e+22]\n",
            "[1.63678131e+23 1.63678131e+23 1.63678131e+23 1.63678131e+23]\n",
            "[2.91447291e+23 2.91447291e+23 2.91447291e+23 2.91447291e+23]\n",
            "[4.16084485e+23 4.16084485e+23 4.16084485e+23 4.16084485e+23]\n",
            "[1.05638056e+24 1.05638056e+24 1.05638056e+24 1.05638056e+24]\n",
            "[2.75468217e+24 2.75468217e+24 2.75468217e+24 2.75468217e+24]\n",
            "[6.2951257e+24 6.2951257e+24 6.2951257e+24 6.2951257e+24]\n",
            "[9.92505479e+24 9.92505479e+24 9.92505479e+24 9.92505479e+24]\n",
            "[1.36301295e+25 1.36301295e+25 1.36301295e+25 1.36301295e+25]\n",
            "[3.11342454e+25 3.11342454e+25 3.11342454e+25 3.11342454e+25]\n",
            "[7.11932553e+25 7.11932553e+25 7.11932553e+25 7.11932553e+25]\n",
            "[1.72100434e+26 1.72100434e+26 1.72100434e+26 1.72100434e+26]\n",
            "[1.15824528e+26 1.15824528e+26 1.15824528e+26 1.15824528e+26]\n",
            "[1.91886819e+26 1.91886819e+26 1.91886819e+26 1.91886819e+26]\n",
            "[2.29123025e+26 2.29123025e+26 2.29123025e+26 2.29123025e+26]\n",
            "[5.62001075e+26 5.62001075e+26 5.62001075e+26 5.62001075e+26]\n",
            "[1.40646709e+27 1.40646709e+27 1.40646709e+27 1.40646709e+27]\n",
            "[9.98158318e+26 9.98158318e+26 9.98158318e+26 9.98158318e+26]\n",
            "[2.86827679e+27 2.86827679e+27 2.86827679e+27 2.86827679e+27]\n",
            "[6.79190737e+27 6.79190737e+27 6.79190737e+27 6.79190737e+27]\n",
            "[1.07148363e+28 1.07148363e+28 1.07148363e+28 1.07148363e+28]\n",
            "[2.43117445e+28 2.43117445e+28 2.43117445e+28 2.43117445e+28]\n",
            "[4.90017613e+28 4.90017613e+28 4.90017613e+28 4.90017613e+28]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "example 0/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[1.64041717 1.62444036 1.84643715 1.74245779]\n",
            "[26.17709666 26.16356372 26.32971596 26.65659503]\n",
            "[1523.84283679 1524.44260348 1524.34367998 1523.8239415 ]\n",
            "[344141.80102695 344141.31189774 344141.17342912 344141.94633328]\n",
            "[2.42784269e+08 2.42784270e+08 2.42784270e+08 2.42784269e+08]\n",
            "[3.84056969e+11 3.84056969e+11 3.84056969e+11 3.84056969e+11]\n",
            "[9.44366787e+14 9.44366787e+14 9.44366787e+14 9.44366787e+14]\n",
            "[3.45650766e+18 3.45650766e+18 3.45650766e+18 3.45650766e+18]\n",
            "[2.15716677e+22 2.15716677e+22 2.15716677e+22 2.15716677e+22]\n",
            "[5.98425979e+26 5.98425979e+26 5.98425979e+26 5.98425979e+26]\n",
            "[4.13298279e+31 4.13298279e+31 4.13298279e+31 4.13298279e+31]\n",
            "[6.34750131e+36 6.34750131e+36 6.34750131e+36 6.34750131e+36]\n",
            "[1.15288107e+42 1.15288107e+42 1.15288107e+42 1.15288107e+42]\n",
            "[3.95789705e+47 3.95789705e+47 3.95789705e+47 3.95789705e+47]\n",
            "[3.07885926e+53 3.07885926e+53 3.07885926e+53 3.07885926e+53]\n",
            "[2.65410191e+59 2.65410191e+59 2.65410191e+59 2.65410191e+59]\n",
            "[5.91296299e+65 5.91296299e+65 5.91296299e+65 5.91296299e+65]\n",
            "[2.60281566e+72 2.60281566e+72 2.60281566e+72 2.60281566e+72]\n",
            "[3.18776648e+79 3.18776648e+79 3.18776648e+79 3.18776648e+79]\n",
            "[1.04882041e+87 1.04882041e+87 1.04882041e+87 1.04882041e+87]\n",
            "[8.55193022e+94 8.55193022e+94 8.55193022e+94 8.55193022e+94]\n",
            "[1.56318844e+103 1.56318844e+103 1.56318844e+103 1.56318844e+103]\n",
            "[6.37254839e+111 6.37254839e+111 6.37254839e+111 6.37254839e+111]\n",
            "[4.82538372e+120 4.82538372e+120 4.82538372e+120 4.82538372e+120]\n",
            "[6.50406478e+129 6.50406478e+129 6.50406478e+129 6.50406478e+129]\n",
            "[1.51560497e+139 1.51560497e+139 1.51560497e+139 1.51560497e+139]\n",
            "[3.1401812e+148 3.1401812e+148 3.1401812e+148 3.1401812e+148]\n",
            "[1.83175721e+158 1.83175721e+158 1.83175721e+158 1.83175721e+158]\n",
            "[2.37918478e+168 2.37918478e+168 2.37918478e+168 2.37918478e+168]\n",
            "[7.41235413e+178 7.41235413e+178 7.41235413e+178 7.41235413e+178]\n",
            "[7.41906247e+189 7.41906247e+189 7.41906247e+189 7.41906247e+189]\n",
            "[1.5415273e+201 1.5415273e+201 1.5415273e+201 1.5415273e+201]\n",
            "[6.95854528e+212 6.95854528e+212 6.95854528e+212 6.95854528e+212]\n",
            "[6.12731983e+224 6.12731983e+224 6.12731983e+224 6.12731983e+224]\n",
            "[1.3490554e+237 1.3490554e+237 1.3490554e+237 1.3490554e+237]\n",
            "[6.30060147e+249 6.30060147e+249 6.30060147e+249 6.30060147e+249]\n",
            "[8.87417409e+262 8.87417409e+262 8.87417409e+262 8.87417409e+262]\n",
            "[3.43954423e+276 3.43954423e+276 3.43954423e+276 3.43954423e+276]\n",
            "[3.21104378e+290 3.21104378e+290 3.21104378e+290 3.21104378e+290]\n",
            "[8.78156852e+304 8.78156852e+304 8.78156852e+304 8.78156852e+304]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n",
            "[inf inf inf inf]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "example 1/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0.         0.02375976 0.         0.        ]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 2/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 3/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 4/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 5/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 6/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 7/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 8/22641 completed\n",
            "[0.78511972 0.23882618 0.15468438 0.30820297]\n",
            "[0. 0. 0. 0.]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "[nan nan nan nan]\n",
            "example 9/22641 completed\n",
            "epoch 0 completed\n",
            "average vertical loss [nan nan nan ... nan nan nan]\n",
            "average horizontal loss [nan nan nan nan]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in add\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXWUbBTxo7pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_predict(model):\n",
        "  x,y1,a,waa,wya,wax,ba,by,outputs=model_params_init()\n",
        "  waa,wax,wya,ba,by,a=model\n",
        "  x=get_one_hot_encoded_vector(0)#i th values sentece chosen \n",
        "  #forward pass for a single sentence\n",
        "  for t in range(0,no_of_layers):\n",
        "    a[:,t+1],y1[:,t],loss1,loss2=forward_pass(a[:,t],x[:,t],wax[:,:,t],wya[:,:,t],waa[:,t],ba[0,t],by[0,t] )#perform forward pass at instant t\n",
        "    #total_loss1.append(loss1)\n",
        "    #total_loss2.append(loss2)\n",
        "  print('actual sentece: '+ vector_to_sentence(x))\n",
        "  print('predicted sentece: '+ vector_to_sentence(y1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rvR2eThpn_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "ec432e59-c0ea-41f3-fd70-29f1e9adadf2"
      },
      "source": [
        "model_predict(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actual sentece: absolutely wonderful silky and sexy and comfortable O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
            "predicted sentece: firstly had sucked very everyth matetial all prim stitches orde skinnys fabulous restriction coke resisted pictu i i fan O petite bc crunchy O O O O O this O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O sink \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67IEgJkO24So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#output vector to sentence\n",
        "def vector_to_sentence(y):\n",
        "  key_list = list(dictionary.keys()) \n",
        "  val_list = list(dictionary.values())\n",
        "  op_sentence=''\n",
        "  for i in range(no_of_layers):\n",
        "    avec=y[:,i]\n",
        "    word_value=np.argmax(avec)\n",
        "    word=key_list[val_list.index(word_value)]\n",
        "    #print(word)\n",
        "    op_sentence=op_sentence+word+\" \"\n",
        "  return op_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHHUntRTTim",
        "colab_type": "text"
      },
      "source": [
        "# Task 5 The input sentence vector goes into each layer and does forward pass ->Write Forward and Backward pass\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7LJwqHFRv5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(a,x,wax,wya,waa,ba,by):\n",
        "  #at timestep t\n",
        "\n",
        "  #a[t],x[t],wax,wya,waa,ba,by-->a,x\n",
        "  #a[t+1],y[t+1]-->a_next,y_next\n",
        "  a_next = np.tanh(waa*a+wax*x+ba)\n",
        "  wya=np.expand_dims(wya,axis=1)\n",
        "  s_inp=np.dot(wya,a[np.newaxis])+by\n",
        "  y_next = softmax(s_inp)\n",
        "\n",
        "  #y_next = softmax(np.dot(wya,a)+by)\n",
        "  loss1 = - x*np.log(y_next) - ((1-x)*np.log(1-y_next))#y[t]=x[t] and ycap[t]=y[t+1]\n",
        "  loss2 = a_next*np.log(a) #y=a[t+1] and ycap=a[t]\n",
        "\n",
        "  print(a_next.shape)\n",
        "  print(y_next.shape)\n",
        "  return a_next,y_next,loss1,loss2\n",
        "\n",
        "def backward_pass(a,a_next,y_next,x):\n",
        "  #a[t],x[t],wax,wya,waa,ba,by-->a,x\n",
        "  #a[t+1],y[t+1]-->a_next,y_next\n",
        "  \n",
        "  #differentiation of loss function wrt a[t]\n",
        "  da = np.log(a) #dloss2/da[t+1]\n",
        "  dy = x/y_next - ( np.log(1-y_next) * (1-x[t]/1-y_next) )#dloss1/dy[t+1])\n",
        "  dwaa = da*((1-(a_next**2))*a) #dloss2/dwaa waa of time step t\n",
        "  dwax = da*((1-(a_next**2))*x) #dloss2/dwax\n",
        "  dwya = dy*y_next*(1-y_next)*a #dloss1/dwya ->dloss1/dy*dy/dwya\n",
        "  dba = da*(1-(a_next**2)) #dloss2/dba\n",
        "  dby = dy*y_next*(1-y_next) #dloss1/dby\n",
        "  return dwaa,dwya,dwax,dba,dby"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0Ymkg24dK1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}